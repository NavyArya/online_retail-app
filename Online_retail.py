# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11GpSuzS-ZaSp3unbCA6i6Z7MM1UEzJsP
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from datetime import timedelta

from google.colab import files
uploaded = files.upload()  # Upload your CSV dataset

df= pd.read_csv('online_retail.csv')
# Show initial shape of the data
print(f"Initial data shape: {df.shape}")

# Data Preprocessing
# 1. Remove rows with missing CustomerID
df = df.dropna(subset=['CustomerID'])
print(f"After dropping missing CustomerID rows: {df.shape}")

# 2. Exclude cancelled invoices
# Cancelled invoices have InvoiceNo starting with 'C'
df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]
print(f"After removing cancelled invoices: {df.shape}")

# 3. Remove rows with negative or zero Quantity or UnitPrice
df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]
print(f"After removing rows with non-positive Quantity or UnitPrice: {df.shape}")

# Reset index after filtering
df = df.reset_index(drop=True)

# Verify final cleaned data
print(df.head())

# Check for missing values
print(df.isnull().sum())

# Drop (or fill) missing values
df = df.dropna()  # or df.fillna(0) or use more advanced imputing

# Fix data types if needed (dates, numbers)
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])  # Example for date column
df['UnitPrice'] = pd.to_numeric(df['UnitPrice'], errors='coerce')  # Example for numeric amount

# EDA(Exploratory Data Analysis)
# 1. Transaction Volume by Country
transaction_volume_country = df.groupby('Country')['InvoiceNo'].nunique().sort_values(ascending=False)
plt.figure(figsize=(12,6))
sns.barplot(x=transaction_volume_country.index, y=transaction_volume_country.values, palette='coolwarm')
plt.xticks(rotation=90)
plt.title('Transaction Volume by Country')
plt.ylabel('Number of Unique Transactions')
plt.xlabel('Country')
plt.show()

# 2. Top-Selling Products (by Quantity)
top_products = df.groupby('Description')['Quantity'].sum().sort_values(ascending=False).head(15)
plt.figure(figsize=(12,6))
sns.barplot(x=top_products.values, y=top_products.index, palette='viridis')
plt.title('Top 15 Best-Selling Products (by Quantity)')
plt.xlabel('Total Quantity Sold')
plt.ylabel('Product')
plt.show()

# 3. Purchase Trends Over Time (Monthly Sales Amount)
df['InvoiceMonth'] = df['InvoiceDate'].dt.to_period('M')
monthly_sales = df.groupby('InvoiceMonth').apply(lambda x: (x['Quantity'] * x['UnitPrice']).sum())
monthly_sales.index = monthly_sales.index.to_timestamp()  # Convert PeriodIndex to Timestamp for plotting
plt.figure(figsize=(14,6))
sns.lineplot(x=monthly_sales.index, y=monthly_sales.values, marker='o')
plt.title('Monthly Sales Trend Over Time')
plt.xlabel('Month')
plt.ylabel('Total Sales (Quantity x UnitPrice)')
plt.grid(True)
plt.show()

# 4a. Monetary Distribution per Transaction
df['TotalPrice'] = df['Quantity'] * df['UnitPrice']
transaction_total = df.groupby('InvoiceNo')['TotalPrice'].sum()
plt.figure(figsize=(8,5))
sns.histplot(transaction_total, bins=50, kde=True)
plt.title('Distribution of Total Price per Transaction')
plt.xlabel('Total Price')
plt.ylabel('Frequency')
plt.show()

# 4b. Monetary Distribution per Customer
customer_total = df.groupby('CustomerID')['TotalPrice'].sum()
plt.figure(figsize=(8,5))
sns.histplot(customer_total, bins=50, kde=True)
plt.title('Distribution of Total Price per Customer')
plt.xlabel('Total Price')
plt.ylabel('Frequency')
plt.show()

5. # RFM Distribution
# Assume reference date is one day after last transaction date
reference_date = df['InvoiceDate'].max() + pd.Timedelta(days=1)

# Recency = days since last purchase
recency_df = df.groupby('CustomerID')['InvoiceDate'].max().reset_index()
recency_df['Recency'] = (reference_date - recency_df['InvoiceDate']).dt.days

# Frequency = number of distinct invoices
frequency_df = df.groupby('CustomerID')['InvoiceNo'].nunique().reset_index()
frequency_df.rename(columns={'InvoiceNo':'Frequency'}, inplace=True)

# Monetary = total spending
monetary_df = df.groupby('CustomerID')['TotalPrice'].sum().reset_index()
monetary_df.rename(columns={'TotalPrice':'Monetary'}, inplace=True)

# Merge R, F, M
rfm = recency_df.merge(frequency_df, on='CustomerID').merge(monetary_df, on='CustomerID')

# Plot distributions
fig, axes = plt.subplots(1, 3, figsize=(18,5))

sns.histplot(rfm['Recency'], bins=30, ax=axes[0], kde=True, color='skyblue')
axes[0].set_title('Recency Distribution (days since last purchase)')
axes[0].set_xlabel('Days')

sns.histplot(rfm['Frequency'], bins=30, ax=axes[1], kde=True, color='lightgreen')
axes[1].set_title('Frequency Distribution (number of purchases)')
axes[1].set_xlabel('Frequency')

sns.histplot(rfm['Monetary'], bins=30, ax=axes[2], kde=True, color='salmon')
axes[2].set_title('Monetary Distribution (total spend)')
axes[2].set_xlabel('Amount')

plt.tight_layout()
plt.show()

# 6. Elbow Curve for KMeans Cluster Selection (using scaled RFM)
scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(rfm[['Recency','Frequency','Monetary']])

inertia = []
K_range = range(1, 11)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(rfm_scaled)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(8,5))
plt.plot(K_range, inertia, 'bo-')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method For Optimal Number of Clusters')
plt.xticks(K_range)
plt.grid(True)
plt.show()

# 7. Customer Cluster Profiles

# Example: choose k=4 clusters from elbow method
kmeans_final = KMeans(n_clusters=4, random_state=42, n_init=10)
rfm['Cluster'] = kmeans_final.fit_predict(rfm_scaled)

cluster_profiles = rfm.groupby('Cluster').agg({
    'Recency':'mean',
    'Frequency':'mean',
    'Monetary':['mean','count']
}).round(2)
cluster_profiles.columns = ['Avg Recency', 'Avg Frequency', 'Avg Monetary', 'Customer Count']
print(cluster_profiles)

# You can visualize clusters:

plt.figure(figsize=(10,7))
sns.scatterplot(data=rfm, x='Recency', y='Monetary', hue='Cluster', palette='Set2', s=70)
plt.title("Clusters Based on RFM (Recency vs Monetary)")
plt.xlabel('Recency (days)')
plt.ylabel('Monetary (Total Spend)')
plt.legend(title='Cluster')
plt.show()

# 8. Product Recommendation Heatmap / Similarity Matrix
# Create a customer-product pivot table with total quantity purchased
pivot_table = df.pivot_table(index='CustomerID', columns='StockCode', values='Quantity', aggfunc='sum', fill_value=0)
# Compute product-product cosine similarity matrix
product_matrix = pivot_table.T  # transpose to product by customer
similarity_matrix = cosine_similarity(product_matrix)
similarity_df = pd.DataFrame(similarity_matrix, index=product_matrix.index, columns=product_matrix.index)
# Plot heatmap of product similarity for a subset (e.g. top 30 products by quantity sold)
top_30_products = df.groupby('StockCode')['Quantity'].sum().sort_values(ascending=False).head(30).index
plt.figure(figsize=(12,10))
sns.heatmap(similarity_df.loc[top_30_products, top_30_products], cmap='viridis')
plt.title("Product Similarity Heatmap (Cosine Similarity)")
plt.show()

# Calculate recency, frequency, monetary
# Assuming df is your preprocessed dataset with columns:
# 'CustomerID', 'InvoiceDate', 'InvoiceNo', and 'TotalPrice' (Quantity*UnitPrice)

# Define reference date (usually 1 day after last transaction date)
ref_date = df['InvoiceDate'].max() + timedelta(days=1)

# Calculate RFM metrics per customer:
rfm = df.groupby('CustomerID').agg({
    'InvoiceDate': lambda x: (ref_date - x.max()).days,  # Recency: days since last purchase
    'InvoiceNo': 'nunique',                              # Frequency: number of unique transactions
    'TotalPrice': 'sum'                                  # Monetary: total amount spent
}).rename(columns={'InvoiceDate': 'Recency', 'InvoiceNo': 'Frequency', 'TotalPrice': 'Monetary'})

print(rfm.head())

# Standardize/Normalize RFM features
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(rfm)

# Kmeans clustering using elbow and silhoutte score method
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

inertia = []
silhouette = []
K_range = range(2, 11)  # Check clusters from 2 to 10

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(rfm_scaled)
    inertia.append(kmeans.inertia_)
    silhouette.append(silhouette_score(rfm_scaled, kmeans.labels_))

# Plot elbow and silhouette
fig, ax1 = plt.subplots(figsize=(10,6))
ax2 = ax1.twinx()

ax1.plot(K_range, inertia, 'bo-', label='Inertia (Elbow)')
ax2.plot(K_range, silhouette, 'rs-', label='Silhouette')

ax1.set_xlabel('Number of clusters k')
ax1.set_ylabel('Inertia', color='b')
ax2.set_ylabel('Silhouette Score', color='r')
plt.title('KMeans: Elbow Method & Silhouette Score for k selection')
ax1.legend(loc='upper left')
ax2.legend(loc='upper right')
plt.show()

#Choose the best number of clusters (say best_k=4 based on the elbow/silhouette plots):
best_k = 4  # Adjust after interpreting elbow/silhouette plots

kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
clusters = kmeans.fit_predict(rfm_scaled)

# Add cluster labels to RFM DataFrame
rfm['Cluster'] = clusters

print(rfm['Cluster'].value_counts())

# Calculate average Recency, Frequency, Monetary per cluster to understand them
cluster_summary = rfm.groupby('Cluster').mean().round(2)
print(cluster_summary)

# Define heuristics to assign segment names:
def label_cluster(row):
    if (row['Recency'] <= cluster_summary['Recency'].median() and
        row['Frequency'] >= cluster_summary['Frequency'].median() and
        row['Monetary'] >= cluster_summary['Monetary'].median()):
        return 'High-Value'
    elif (row['Frequency'] >= cluster_summary['Frequency'].median() and
          row['Monetary'] >= cluster_summary['Monetary'].median()):
        return 'Regular'
    elif (row['Recency'] > cluster_summary['Recency'].median() and
          row['Frequency'] < cluster_summary['Frequency'].median()):
        return 'Occasional'
    else:
        return 'At-Risk'

rfm['Segment'] = rfm.apply(label_cluster, axis=1)

print(rfm['Segment'].value_counts())
print(rfm.head())

# DBscan clustering
from sklearn.neighbors import NearestNeighbors
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score
import numpy as np

# Plot k-distance to select eps
k = 4  # Min points parameter, usually dimensionality + 1
nbrs = NearestNeighbors(n_neighbors=k).fit(rfm_scaled)
distances, indices = nbrs.kneighbors(rfm_scaled)
distances = np.sort(distances[:, k-1])

plt.figure(figsize=(8,5))
plt.plot(distances)
plt.title('k-distance Graph for Choosing eps in DBSCAN')
plt.xlabel('Sorted Points')
plt.ylabel(f'{k}th Nearest Neighbor Distance')
plt.grid(True)
plt.show()

# After determining eps visually (example eps=0.8), run DBSCAN
eps = 0.8
min_samples = k

dbscan = DBSCAN(eps=eps, min_samples=min_samples)
dbscan_labels = dbscan.fit_predict(rfm_scaled)

n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
print(f"DBSCAN found clusters (excluding noise): {n_clusters}")
print(f"Cluster labels (-1 means noise): {np.unique(dbscan_labels)}")

# Silhouette score excluding noise
mask = dbscan_labels != -1
if len(set(dbscan_labels[mask])) > 1:
    sil_score = silhouette_score(rfm_scaled[mask], dbscan_labels[mask])
    print(f"Silhouette score for DBSCAN (excluding noise): {sil_score:.3f}")
else:
    print("Not enough clusters to compute silhouette score excluding noise")

# Hierarchical clustering
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

# Plot dendrogram to decide the number of clusters
linked = linkage(rfm_scaled, method='ward')
plt.figure(figsize=(12, 7))
dendrogram(linked, truncate_mode='level', p=5)
plt.title('Hierarchical Clustering Dendrogram (Truncated)')
plt.xlabel('Sample Index')
plt.ylabel('Distance')
plt.show()

# Instantiate model with chosen number of clusters
n_clusters = 4  # adjust after dendrogram inspection
agglo = AgglomerativeClustering(n_clusters=n_clusters)
clusters_agglo = agglo.fit_predict(rfm_scaled)

print(f"Clusters assigned by Hierarchical Clustering: {pd.Series(clusters_agglo).value_counts()}")

rfm['KMeans_Cluster'] = clusters
rfm['DBSCAN_Cluster'] = dbscan_labels
rfm['Agglo_Cluster'] = clusters_agglo

print(rfm.head())

# Print best cluster count by silhouette score
max_silhouette = max(silhouette)
best_k = K_range[silhouette.index(max_silhouette)]
print(f"Best number of clusters based on silhouette score: {best_k} (Silhouette Score: {max_silhouette:.3f})")

#  Run Clustering and Label Segments
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Assume 'rfm' is your DataFrame with columns: 'Recency', 'Frequency', 'Monetary'

# 1. Standardize RFM data for clustering
scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(rfm[['Recency', 'Frequency', 'Monetary']])

# 2. Run KMeans with 4 clusters (as per your segmentation)
kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
rfm['Cluster'] = kmeans.fit_predict(rfm_scaled)

# 3. Calculate average RFM values per cluster to interpret
cluster_avg = rfm.groupby('Cluster')[['Recency', 'Frequency', 'Monetary']].mean().round(1)
print("Cluster average RFM values:")
print(cluster_avg)

# 4. Map clusters to segment labels based on average RFM values
# You might need to explore cluster_avg manually to match clusters correctly;
# here is a generic approach using thresholds:

def assign_segment(row, cluster_avg_df):
    # Get cluster averages to define high/medium/low thresholds
    recency_med = cluster_avg_df['Recency'].median()
    frequency_med = cluster_avg_df['Frequency'].median()
    monetary_med = cluster_avg_df['Monetary'].median()

    if (row['Recency'] <= recency_med) and (row['Frequency'] >= frequency_med) and (row['Monetary'] >= monetary_med):
        return 'High-Value'
    elif (row['Frequency'] >= frequency_med) and (row['Monetary'] >= monetary_med):
        return 'Regular'
    elif (row['Frequency'] < frequency_med) and (row['Monetary'] < monetary_med) and (row['Recency'] > recency_med):
        return 'Occasional'
    else:
        return 'At-Risk'

rfm['Segment'] = rfm.apply(assign_segment, axis=1, cluster_avg_df=cluster_avg)

# 5. Check count of customers in each segment
print("\nCustomer count per segment:")
print(rfm['Segment'].value_counts())

# Optional: View sample records
print("\nSample of labeled RFM data:")
print(rfm.head())

# 2D scatter plot
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 7))
sns.scatterplot(
    x='Recency',
    y='Monetary',
    hue='Segment',  # or 'Cluster' if you haven't assigned segment labels yet
    data=rfm,
    palette='Set2',
    alpha=0.7,
    s=70
)
plt.gca().invert_xaxis()  # Optional: more recent customers appear on left side
plt.title('Customer Segments based on Recency and Monetary Value')
plt.xlabel('Recency (Days Since Last Purchase)')
plt.ylabel('Monetary (Total Spend)')
plt.legend(title='Segment')
plt.show()

# 3D scatter plot
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(12, 9))
ax = fig.add_subplot(111, projection='3d')

# Assign colors to segments automatically
segments = rfm['Segment'].unique()
colors = sns.color_palette('Set2', n_colors=len(segments))
color_map = dict(zip(segments, colors))

for segment in segments:
    mask = rfm['Segment'] == segment
    ax.scatter(
        rfm.loc[mask, 'Recency'],
        rfm.loc[mask, 'Frequency'],
        rfm.loc[mask, 'Monetary'],
        s=60,
        alpha=0.7,
        label=segment,
        color=color_map[segment]
    )

ax.set_xlabel('Recency (Days)')
ax.set_ylabel('Frequency (Number of Purchases)')
ax.set_zlabel('Monetary (Total Spend)')
ax.set_title('3D Visualization of Customer Segments (RFM)')
ax.invert_xaxis()  # Recent customers on left
ax.legend()
plt.show()

